{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "from torchvision.models import densenet121 , DenseNet121_Weights\n",
    "from torchvision.models import densenet161, DenseNet161_Weights\n",
    "from torchvision.models import densenet169, DenseNet169_Weights\n",
    "from torchvision.models import densenet201, DenseNet201_Weights\n",
    "\n",
    "from torchvision.models import efficientnet_v2_s, efficientnet_v2_m, efficientnet_v2_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pool(model, m_type='resnet'):\n",
    "    features = [torch.randn(size=(1, 3, 256, 256))]\n",
    "    if m_type == 'resnet':\n",
    "        modules = list(model.children())\n",
    "        encoder = torch.nn.Sequential(*(list(modules)[:-2]))\n",
    "    else: \n",
    "        encoder = model.features\n",
    "\n",
    "    # get feature output\n",
    "    for layer in encoder:\n",
    "        features.append(layer(features[-1]))\n",
    "    \n",
    "    print('features')\n",
    "    for ix, f in enumerate(features): \n",
    "        print(f\"[features - {ix}] : {f.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res18 = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "res34 = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "res50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "res101 = resnet101(weights=ResNet101_Weights.IMAGENET1K_V1)\n",
    "res152 = resnet152(weights=ResNet152_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[features - 0] : torch.Size([1, 3, 256, 256])\n",
      "[features - 1] : torch.Size([1, 64, 128, 128])\n",
      "[features - 2] : torch.Size([1, 64, 128, 128])\n",
      "[features - 3] : torch.Size([1, 64, 128, 128])\n",
      "[features - 4] : torch.Size([1, 64, 64, 64])\n",
      "[features - 5] : torch.Size([1, 64, 64, 64])\n",
      "[features - 6] : torch.Size([1, 128, 32, 32])\n",
      "[features - 7] : torch.Size([1, 256, 16, 16])\n",
      "[features - 8] : torch.Size([1, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "show_pool(res18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[features - 0] : torch.Size([1, 3, 256, 256])\n",
      "[features - 1] : torch.Size([1, 64, 128, 128])\n",
      "[features - 2] : torch.Size([1, 64, 128, 128])\n",
      "[features - 3] : torch.Size([1, 64, 128, 128])\n",
      "[features - 4] : torch.Size([1, 64, 64, 64])\n",
      "[features - 5] : torch.Size([1, 64, 64, 64])\n",
      "[features - 6] : torch.Size([1, 128, 32, 32])\n",
      "[features - 7] : torch.Size([1, 256, 16, 16])\n",
      "[features - 8] : torch.Size([1, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "show_pool(res34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[features - 0] : torch.Size([1, 3, 256, 256])\n",
      "[features - 1] : torch.Size([1, 64, 128, 128])\n",
      "[features - 2] : torch.Size([1, 64, 128, 128])\n",
      "[features - 3] : torch.Size([1, 64, 128, 128])\n",
      "[features - 4] : torch.Size([1, 64, 64, 64])\n",
      "[features - 5] : torch.Size([1, 256, 64, 64])\n",
      "[features - 6] : torch.Size([1, 512, 32, 32])\n",
      "[features - 7] : torch.Size([1, 1024, 16, 16])\n",
      "[features - 8] : torch.Size([1, 2048, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "show_pool(res50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[features - 0] : torch.Size([1, 3, 256, 256])\n",
      "[features - 1] : torch.Size([1, 64, 128, 128])\n",
      "[features - 2] : torch.Size([1, 64, 128, 128])\n",
      "[features - 3] : torch.Size([1, 64, 128, 128])\n",
      "[features - 4] : torch.Size([1, 64, 64, 64])\n",
      "[features - 5] : torch.Size([1, 256, 64, 64])\n",
      "[features - 6] : torch.Size([1, 512, 32, 32])\n",
      "[features - 7] : torch.Size([1, 1024, 16, 16])\n",
      "[features - 8] : torch.Size([1, 2048, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "show_pool(res101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[features - 0] : torch.Size([1, 3, 256, 256])\n",
      "[features - 1] : torch.Size([1, 64, 128, 128])\n",
      "[features - 2] : torch.Size([1, 64, 128, 128])\n",
      "[features - 3] : torch.Size([1, 64, 128, 128])\n",
      "[features - 4] : torch.Size([1, 64, 64, 64])\n",
      "[features - 5] : torch.Size([1, 256, 64, 64])\n",
      "[features - 6] : torch.Size([1, 512, 32, 32])\n",
      "[features - 7] : torch.Size([1, 1024, 16, 16])\n",
      "[features - 8] : torch.Size([1, 2048, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "show_pool(res152)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense121 = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
    "dense161 = densenet161(weights=DenseNet161_Weights.IMAGENET1K_V1)\n",
    "dense169 = densenet169(weights=DenseNet169_Weights.IMAGENET1K_V1)\n",
    "dense201 = densenet201(weights=DenseNet201_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[features - 0] : torch.Size([1, 3, 256, 256])\n",
      "[features - 1] : torch.Size([1, 64, 128, 128])\n",
      "[features - 2] : torch.Size([1, 64, 128, 128])\n",
      "[features - 3] : torch.Size([1, 64, 128, 128])\n",
      "[features - 4] : torch.Size([1, 64, 64, 64])\n",
      "[features - 5] : torch.Size([1, 256, 64, 64])\n",
      "[features - 6] : torch.Size([1, 128, 32, 32])\n",
      "[features - 7] : torch.Size([1, 512, 32, 32])\n",
      "[features - 8] : torch.Size([1, 256, 16, 16])\n",
      "[features - 9] : torch.Size([1, 1024, 16, 16])\n",
      "[features - 10] : torch.Size([1, 512, 8, 8])\n",
      "[features - 11] : torch.Size([1, 1024, 8, 8])\n",
      "[features - 12] : torch.Size([1, 1024, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "show_pool(dense121, m_type='densenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[features - 0] : torch.Size([1, 3, 256, 256])\n",
      "[features - 1] : torch.Size([1, 96, 128, 128])\n",
      "[features - 2] : torch.Size([1, 96, 128, 128])\n",
      "[features - 3] : torch.Size([1, 96, 128, 128])\n",
      "[features - 4] : torch.Size([1, 96, 64, 64])\n",
      "[features - 5] : torch.Size([1, 384, 64, 64])\n",
      "[features - 6] : torch.Size([1, 192, 32, 32])\n",
      "[features - 7] : torch.Size([1, 768, 32, 32])\n",
      "[features - 8] : torch.Size([1, 384, 16, 16])\n",
      "[features - 9] : torch.Size([1, 2112, 16, 16])\n",
      "[features - 10] : torch.Size([1, 1056, 8, 8])\n",
      "[features - 11] : torch.Size([1, 2208, 8, 8])\n",
      "[features - 12] : torch.Size([1, 2208, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "show_pool(dense161, m_type='densenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[features - 0] : torch.Size([1, 3, 256, 256])\n",
      "[features - 1] : torch.Size([1, 64, 128, 128])\n",
      "[features - 2] : torch.Size([1, 64, 128, 128])\n",
      "[features - 3] : torch.Size([1, 64, 128, 128])\n",
      "[features - 4] : torch.Size([1, 64, 64, 64])\n",
      "[features - 5] : torch.Size([1, 256, 64, 64])\n",
      "[features - 6] : torch.Size([1, 128, 32, 32])\n",
      "[features - 7] : torch.Size([1, 512, 32, 32])\n",
      "[features - 8] : torch.Size([1, 256, 16, 16])\n",
      "[features - 9] : torch.Size([1, 1280, 16, 16])\n",
      "[features - 10] : torch.Size([1, 640, 8, 8])\n",
      "[features - 11] : torch.Size([1, 1664, 8, 8])\n",
      "[features - 12] : torch.Size([1, 1664, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "show_pool(dense169, m_type='densenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[features - 0] : torch.Size([1, 3, 256, 256])\n",
      "[features - 1] : torch.Size([1, 64, 128, 128])\n",
      "[features - 2] : torch.Size([1, 64, 128, 128])\n",
      "[features - 3] : torch.Size([1, 64, 128, 128])\n",
      "[features - 4] : torch.Size([1, 64, 64, 64])\n",
      "[features - 5] : torch.Size([1, 256, 64, 64])\n",
      "[features - 6] : torch.Size([1, 128, 32, 32])\n",
      "[features - 7] : torch.Size([1, 512, 32, 32])\n",
      "[features - 8] : torch.Size([1, 256, 16, 16])\n",
      "[features - 9] : torch.Size([1, 1792, 16, 16])\n",
      "[features - 10] : torch.Size([1, 896, 8, 8])\n",
      "[features - 11] : torch.Size([1, 1920, 8, 8])\n",
      "[features - 12] : torch.Size([1, 1920, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "show_pool(dense201, m_type='densenet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_s_v2 = efficientnet_v2_s(pretrained=True)\n",
    "ef_m_v2 = efficientnet_v2_m(pretrained=True)\n",
    "ef_l_v2 = efficientnet_v2_l(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[features - 0] : torch.Size([1, 3, 256, 256])\n",
      "[features - 1] : torch.Size([1, 24, 128, 128])\n",
      "[features - 2] : torch.Size([1, 24, 128, 128])\n",
      "[features - 3] : torch.Size([1, 48, 64, 64])\n",
      "[features - 4] : torch.Size([1, 64, 32, 32])\n",
      "[features - 5] : torch.Size([1, 128, 16, 16])\n",
      "[features - 6] : torch.Size([1, 160, 16, 16])\n",
      "[features - 7] : torch.Size([1, 256, 8, 8])\n",
      "[features - 8] : torch.Size([1, 1280, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "show_pool(ef_s_v2, m_type='efficientNetV2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[features - 0] : torch.Size([1, 3, 256, 256])\n",
      "[features - 1] : torch.Size([1, 24, 128, 128])\n",
      "[features - 2] : torch.Size([1, 24, 128, 128])\n",
      "[features - 3] : torch.Size([1, 48, 64, 64])\n",
      "[features - 4] : torch.Size([1, 80, 32, 32])\n",
      "[features - 5] : torch.Size([1, 160, 16, 16])\n",
      "[features - 6] : torch.Size([1, 176, 16, 16])\n",
      "[features - 7] : torch.Size([1, 304, 8, 8])\n",
      "[features - 8] : torch.Size([1, 512, 8, 8])\n",
      "[features - 9] : torch.Size([1, 1280, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "show_pool(ef_m_v2, m_type='efficientNetV2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[features - 0] : torch.Size([1, 3, 256, 256])\n",
      "[features - 1] : torch.Size([1, 32, 128, 128])\n",
      "[features - 2] : torch.Size([1, 32, 128, 128])\n",
      "[features - 3] : torch.Size([1, 64, 64, 64])\n",
      "[features - 4] : torch.Size([1, 96, 32, 32])\n",
      "[features - 5] : torch.Size([1, 192, 16, 16])\n",
      "[features - 6] : torch.Size([1, 224, 16, 16])\n",
      "[features - 7] : torch.Size([1, 384, 8, 8])\n",
      "[features - 8] : torch.Size([1, 640, 8, 8])\n",
      "[features - 9] : torch.Size([1, 1280, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "show_pool(ef_l_v2, m_type='efficientNetV2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
